{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e80358a-ddc3-487c-9344-34533aada78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import torch\n",
    "\n",
    "import tianshou as ts\n",
    "\n",
    "import TeachMyAgent as tma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8bbed-1ccd-45ae-a4c0-6f78b0d05704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c77e37-d7ee-4a3b-801d-59e2766032c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebb147-8c97-45f4-9a0a-701d7206c87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae2f687-a9c9-42ed-a9c4-8dbb3d8d9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.policy import SACPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9263f-72c3-4434-8e68-e27b1f1879df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3e23f-3ffc-42e5-bf01-e76e4740e834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fe4a3-826f-4a76-a5ba-671de81c4e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16e1b62-7060-4bfb-be27-e1f862281cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--task', type=str, default=\"BipedalWalkerHardcore-v3\")\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--buffer-size', type=int, default=1000000)\n",
    "    parser.add_argument('--actor-lr', type=float, default=3e-4)\n",
    "    parser.add_argument('--critic-lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--tau', type=float, default=0.005)\n",
    "    parser.add_argument('--alpha', type=float, default=0.1)\n",
    "    parser.add_argument('--auto-alpha', type=int, default=1)\n",
    "    parser.add_argument('--alpha-lr', type=float, default=3e-4)\n",
    "    parser.add_argument('--epoch', type=int, default=100)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=100000)\n",
    "    parser.add_argument('--step-per-collect', type=int, default=10)\n",
    "    parser.add_argument('--update-per-step', type=float, default=0.1)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128])\n",
    "    parser.add_argument('--training-num', type=int, default=10)\n",
    "    parser.add_argument('--test-num', type=int, default=100)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.)\n",
    "    parser.add_argument('--n-step', type=int, default=4)\n",
    "    parser.add_argument(\n",
    "        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    parser.add_argument('--resume-path', type=str, default=None)\n",
    "    return parser.parse_args(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393725f-8f9d-4b63-b6b7-27d3e03cf97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef00e180-ee07-4740-8bcd-86e27c2873ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(gym.Wrapper):\n",
    "    \"\"\"Env wrapper for reward scale, action repeat and removing done penalty\"\"\"\n",
    "\n",
    "    def __init__(self, env, action_repeat=3, reward_scale=5, rm_done=True):\n",
    "        super().__init__(env)\n",
    "        self.action_repeat = action_repeat\n",
    "        self.reward_scale = reward_scale\n",
    "        self.rm_done = rm_done\n",
    "\n",
    "    def step(self, action):\n",
    "        r = 0.0\n",
    "        for _ in range(self.action_repeat):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            # remove done reward penalty\n",
    "            if not done or not self.rm_done:\n",
    "                r = r + reward\n",
    "            if done:\n",
    "                break\n",
    "        # scale reward\n",
    "        return obs, self.reward_scale * r, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34da265d-a8a4-456f-ac43-ac17b6abb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "env = Wrapper(gym.make(args.task))\n",
    "\n",
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "args.max_action = env.action_space.high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eed7f04-a274-49ff-ae15-47b10e38a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = SubprocVectorEnv(\n",
    "    [lambda: Wrapper(gym.make(args.task)) for _ in range(args.training_num)]\n",
    ")\n",
    "# test_envs = gym.make(args.task)\n",
    "test_envs = SubprocVectorEnv(\n",
    "    [\n",
    "        lambda: Wrapper(gym.make(args.task), reward_scale=1, rm_done=False)\n",
    "        for _ in range(args.test_num)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "train_envs.seed(args.seed)\n",
    "test_envs.seed(args.seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76cdec3e-b83b-4860-895b-444948616c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "net_a = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "actor = ActorProb(\n",
    "    net_a,\n",
    "    args.action_shape,\n",
    "    max_action=args.max_action,\n",
    "    device=args.device,\n",
    "    unbounded=True\n",
    ").to(args.device)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "\n",
    "net_c1 = Net(\n",
    "    args.state_shape,\n",
    "    args.action_shape,\n",
    "    hidden_sizes=args.hidden_sizes,\n",
    "    concat=True,\n",
    "    device=args.device\n",
    ")\n",
    "critic1 = Critic(net_c1, device=args.device).to(args.device)\n",
    "critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "\n",
    "net_c2 = Net(\n",
    "    args.state_shape,\n",
    "    args.action_shape,\n",
    "    hidden_sizes=args.hidden_sizes,\n",
    "    concat=True,\n",
    "    device=args.device\n",
    ")\n",
    "critic2 = Critic(net_c2, device=args.device).to(args.device)\n",
    "critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "\n",
    "if args.auto_alpha:\n",
    "    target_entropy = -np.prod(env.action_space.shape)\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=args.device)\n",
    "    alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "    args.alpha = (target_entropy, log_alpha, alpha_optim)\n",
    "\n",
    "policy = SACPolicy(\n",
    "    actor,\n",
    "    actor_optim,\n",
    "    critic1,\n",
    "    critic1_optim,\n",
    "    critic2,\n",
    "    critic2_optim,\n",
    "    tau=args.tau,\n",
    "    gamma=args.gamma,\n",
    "    alpha=args.alpha,\n",
    "    estimation_step=args.n_step,\n",
    "    action_space=env.action_space\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b122f9-345d-4f2e-bc88-d62348e9aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a previous policy\n",
    "if args.resume_path:\n",
    "    policy.load_state_dict(torch.load(args.resume_path))\n",
    "    print(\"Loaded agent from: \", args.resume_path)\n",
    "\n",
    "# collector\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "    exploration_noise=True\n",
    ")\n",
    "test_collector = Collector(policy, test_envs)\n",
    "# train_collector.collect(n_step=args.buffer_size)\n",
    "# log\n",
    "log_path = os.path.join(args.logdir, args.task, 'sac')\n",
    "# writer = SummaryWriter(log_path)\n",
    "# logger = TensorboardLogger(writer)\n",
    "\n",
    "def save_fn(policy):\n",
    "    print('saving')\n",
    "    # torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a562d4-3b77-4d34-878c-f45bdbfc57da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 100001it [08:11, 203.33it/s, alpha=0.104, env_step=100000, len=0, loss/actor=-40.498, loss/alpha=-0.782, loss/critic1=176.738, loss/critic2=174.742, n/ep=0, n/st=10, rew=0.00]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -47.791163 ± 29.421405, best_reward: -21.486006 ± 35.335554 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 100001it [07:46, 214.48it/s, alpha=0.199, env_step=200000, len=0, loss/actor=-62.109, loss/alpha=0.093, loss/critic1=266.739, loss/critic2=260.335, n/ep=0, n/st=10, rew=0.00]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -59.329818 ± 34.202553, best_reward: -21.486006 ± 35.335554 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 100001it [06:39, 250.43it/s, alpha=0.227, env_step=300000, len=0, loss/actor=-78.180, loss/alpha=-0.073, loss/critic1=329.360, loss/critic2=334.073, n/ep=0, n/st=10, rew=0.00]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "Epoch #3: test_reward: 2.199228 ± 92.880750, best_reward: 2.199228 ± 92.880750 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 100001it [06:26, 258.63it/s, alpha=0.256, env_step=400000, len=0, loss/actor=-97.203, loss/alpha=-0.015, loss/critic1=435.555, loss/critic2=454.688, n/ep=0, n/st=10, rew=0.00]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "Epoch #4: test_reward: 46.998529 ± 111.960033, best_reward: 46.998529 ± 111.960033 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 100001it [06:33, 253.83it/s, alpha=0.278, env_step=500000, len=0, loss/actor=-115.976, loss/alpha=-0.069, loss/critic1=525.618, loss/critic2=528.997, n/ep=0, n/st=10, rew=0.00]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 33.326299 ± 126.818333, best_reward: 46.998529 ± 111.960033 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 100001it [06:52, 242.20it/s, alpha=0.300, env_step=600000, len=0, loss/actor=-132.136, loss/alpha=0.056, loss/critic1=584.567, loss/critic2=612.025, n/ep=0, n/st=10, rew=0.00]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "Epoch #6: test_reward: 66.028082 ± 125.914366, best_reward: 66.028082 ± 125.914366 in #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 100001it [07:21, 226.51it/s, alpha=0.327, env_step=700000, len=0, loss/actor=-144.947, loss/alpha=-0.186, loss/critic1=659.711, loss/critic2=672.923, n/ep=0, n/st=10, rew=0.00]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 35.044802 ± 105.156601, best_reward: 66.028082 ± 125.914366 in #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 100001it [07:41, 216.63it/s, alpha=0.298, env_step=800000, len=0, loss/actor=-155.390, loss/alpha=-0.189, loss/critic1=678.853, loss/critic2=701.587, n/ep=0, n/st=10, rew=0.00]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 57.743687 ± 125.716524, best_reward: 66.028082 ± 125.914366 in #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 100001it [06:44, 247.49it/s, alpha=0.294, env_step=900000, len=0, loss/actor=-162.459, loss/alpha=0.035, loss/critic1=693.644, loss/critic2=716.336, n/ep=0, n/st=10, rew=0.00]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "Epoch #9: test_reward: 72.107249 ± 134.198899, best_reward: 72.107249 ± 134.198899 in #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10:  12%|#2        | 12050/100000 [00:50<06:06, 239.87it/s, alpha=0.312, env_step=912040, len=0, loss/actor=-165.178, loss/alpha=-0.082, loss/critic1=713.259, loss/critic2=727.375, n/ep=0, n/st=10, rew=0.00]     \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "result = offpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    args.epoch,\n",
    "    args.step_per_epoch,\n",
    "    args.step_per_collect,\n",
    "    args.test_num,\n",
    "    args.batch_size,\n",
    "    update_per_step=args.update_per_step,\n",
    "    test_in_train=False,\n",
    "    stop_fn=stop_fn,\n",
    "    save_fn=save_fn,\n",
    "    # logger=logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df511cc6-c74c-4ef7-8b45-4c7e05d3c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(result)\n",
    "# Let's watch its performance!\n",
    "policy.eval()\n",
    "test_envs.seed(args.seed)\n",
    "test_collector.reset()\n",
    "result = test_collector.collect(n_episode=args.test_num, render=args.render)\n",
    "rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee2b6c-ee25-4750-917b-633482f7f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f7512e-76c4-4550-878f-c259d7467e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "9.8\n",
      "20\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "env.gravity = 20\n",
    "print(env.gravity)\n",
    "env.print_gravity()\n",
    "\n",
    "env.set_gravity(40)\n",
    "print(env.gravity)\n",
    "env.print_gravity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfea05-9008-48f7-b42b-32ed7ec79c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e327ac6b-f121-49db-8b9c-8c9fd434c4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.0\n",
      "(-0.10434375041651943, 0.03906063338627813, 0.21477228615085472, 0.1514375271789289)\n",
      "41.0\n",
      "(-0.08893778710637525, -0.16590066610000895, 0.2140352901972855, 0.6700038126959267)\n",
      "35.0\n",
      "(-0.08357150201210116, -0.17310259752823062, 0.21538388606672643, 0.8313309310765957)\n",
      "31.0\n",
      "(-0.07971880968863711, -0.1780293823019942, 0.21013077059874896, 0.9404867148296607)\n",
      "29.0\n",
      "(-0.07830760800256713, -0.18442663041693377, 0.2190678708087497, 1.0857062251349299)\n",
      "27.0\n",
      "(-0.07643739300199816, -0.18865499807711522, 0.21770639374702178, 1.180067122762774)\n",
      "26.0\n",
      "(-0.07799230889326551, -0.0006236761621742082, 0.23210123800057633, 1.0438558939224978)\n",
      "24.0\n",
      "(-0.07543778581263856, -0.000660368333622291, 0.21537995433144305, 1.0406792730673975)\n",
      "23.0\n",
      "(-0.072625177660847, -0.19896599491844436, 0.21338378626713403, 1.409641667687952)\n",
      "22.0\n",
      "(-0.07354612031091992, -0.006840778303437101, 0.2135435471586821, 1.178639142477889)\n",
      "22.0\n",
      "(-0.07449019971258565, -0.01576179390213972, 0.23471173567526246, 1.384765219844419)\n",
      "21.0\n",
      "(-0.07126364806858164, -0.21114708930632978, 0.22342132801835643, 1.685601587526198)\n",
      "20.0\n",
      "(-0.0718159165707287, -0.016107889588570207, 0.21531988782303754, 1.386565779171096)\n",
      "20.0\n",
      "(-0.0725721565402366, -0.02414647217933985, 0.23226227991988888, 1.5722162307834078)\n",
      "19.0\n",
      "(-0.06902982679382232, -0.21659935018586954, 0.21391885889742124, 1.8041212507068705)\n",
      "19.0\n",
      "(-0.0697022368433344, -0.22415699866801003, 0.2289735482160952, 1.979225498455341)\n",
      "18.0\n",
      "(-0.06989236442003752, -0.025427831428252, 0.21276727257100814, 1.5942749313912883)\n",
      "18.0\n",
      "(-0.07047671560172011, -0.03247548448098744, 0.22583862297531887, 1.7566225646335858)\n",
      "18.0\n",
      "(-0.07108124084848286, -0.03986092143541231, 0.23938714396360242, 1.927795828357399)\n",
      "17.0\n",
      "(-0.06714778763188627, -0.22791973738257074, 0.21229415424879214, 2.0564174835720253)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for gravity in np.linspace(1, 50, 20):\n",
    "    env = gym.make('CartPole-v1')\n",
    " \n",
    "    np.random.seed(0)\n",
    "    env.seed(0)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    eprew = 0\n",
    "    eplen = 0\n",
    "    while not done:\n",
    "        env.set_gravity(gravity)\n",
    "        \n",
    "        obs, rew, done, info = env.step(eplen%2)\n",
    "        eprew += rew\n",
    "        eplen += 1\n",
    "\n",
    "    env.close()\n",
    "    eplen, eprew\n",
    "    \n",
    "    print(eprew)\n",
    "    print(env.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4e696-8bfa-4e48-84c0-4671609b713c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20706cd0-cade-4137-97d8-a83e3966a371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bc413-6c6e-4f24-b994-933c1dc04251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa8332-41a1-4a3f-9394-cabb87b28ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dda14-12f5-4cfa-9488-09a0b536a3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3e3e3-0b37-4723-a582-de42e6cac90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f38af-11eb-4db4-b64a-39c0515f25c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95596b7c-a6fa-4573-8bdc-b4d9ced48b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ebfe9d4-73c6-4ba0-8732-46c86f14413e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13c92faa-3475-402c-9eda-f30431393655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(10)])\n",
    "# test_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(100)])\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "train_envs = gym.make('CartPole-v0')\n",
    "test_envs = gym.make('CartPole-v0')\n",
    "train_envs.gravity = 50\n",
    "test_envs.gravity = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8dd2b0-7e88-4dbf-beca-f1fd51182cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "policy = ts.policy.DQNPolicy(net, optim, discount_factor=0.9, estimation_step=3, target_update_freq=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b819e430-a955-46bf-a9a6-dd01cab2ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5cb9ba1-28d3-4287-acd4-459c7628deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing collector rewards:  9.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akarshkumar0101/miniconda3/envs/tmacenv/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing collector rewards:  199.99\n",
      "Finished training! Test mean returns: 199.99\n"
     ]
    }
   ],
   "source": [
    "# pre-collect at least 5000 transitions with random action before training\n",
    "train_collector.collect(n_step=5000, random=True)\n",
    "\n",
    "policy.set_eps(0.1)\n",
    "for i in range(int(1e6)):  # total step\n",
    "    collect_result = train_collector.collect(n_step=10)\n",
    "\n",
    "    # once if the collected episodes' mean returns reach the threshold,\n",
    "    # or every 1000 steps, we test it on test_collector\n",
    "    if collect_result['rews'].mean() >= env.spec.reward_threshold or i % 1000 == 0:\n",
    "        policy.set_eps(0.05)\n",
    "        result = test_collector.collect(n_episode=100)\n",
    "        print('testing collector rewards: ', result['rews'].mean())\n",
    "        if result['rews'].mean() >= env.spec.reward_threshold:\n",
    "            print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "            break\n",
    "        else:\n",
    "            # back to training eps\n",
    "            policy.set_eps(0.1)\n",
    "\n",
    "    # train policy with a sampled batch data from buffer\n",
    "    losses = policy.update(64, train_collector.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64052b12-ea98-43c0-8786-d790b2c67bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3cb852-f6f7-42eb-87df-d48338d46f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3da15-042e-4ca7-93d1-ac65b456611f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa7a1c4-3a6a-4d51-a3da-4ee4d21e5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "espace = np.linspace(0, 50, 10)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a538441-3d3b-40c3-9fe0-7a080e473ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "espace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d606c8b-b28a-44b6-b3b9-a833f990e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = gym.make('CartPole-v0')\n",
    "test_envs = gym.make('CartPole-v0')\n",
    "\n",
    "train_envs.gravity = 50\n",
    "test_envs.gravity = 50\n",
    "\n",
    "# train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "# test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "def random_policy():\n",
    "    state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    net = Net(state_shape, action_shape)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    policy = ts.policy.DQNPolicy(net, optim, discount_factor=0.9, estimation_step=3, target_update_freq=320)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def train_policy(policy, env_param):\n",
    "    policy.set_eps(0.1)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# scores = np.zeros(espace.shape[:-1])\n",
    "# policies = np.empty(espace.shape[:-1], dtype=object)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     if i<G:\n",
    "#         p_old = random_policy()\n",
    "#     else:\n",
    "#         coor_old = random_coordinate()\n",
    "#         p_old = policies[coor_old]\n",
    "#     # TODO: maybe add random mutation noise to escape local minima?\n",
    "    \n",
    "#     coor_new = random_coordinate()\n",
    "#     p_new = train_policy(p_old, espace[coor_new])\n",
    "    \n",
    "#     score_new = test_policy(p_new)\n",
    "#     if policies[coor_new] is None or score_new > scores[coor_new]:\n",
    "#         scores[coor_new] = score_new\n",
    "#         policies[coor_new] = p_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4a6648-5c11-4030-84ad-e3174dd9d653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'espace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b16d8c9b6f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mespace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'espace' is not defined"
     ]
    }
   ],
   "source": [
    "def test_policy(policy):\n",
    "    policy.set_eps(0.05)\n",
    "    results = []\n",
    "    for e in espace:\n",
    "        print('starting ', e)\n",
    "        # test_envs.gravity = e[0]\n",
    "        test_envs.set_gravity(e[0])\n",
    "        test_envs.reset()\n",
    "        test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)\n",
    "        \n",
    "        result = test_collector.collect(n_episode=500)\n",
    "        results.append(result['rews'])\n",
    "        \n",
    "    results = np.array(results)\n",
    "    return results\n",
    "\n",
    "plt.plot(espace[:, 0], test_policy(random_policy()).mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b724b05e-781e-40e3-89e9-821168e3da1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collector.env.gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d586915-2687-413e-98aa-d09d4ed53311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
